{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Word Embedding\n",
    "\n",
    "In this exercise, you will work on the skip-gram neural network architecture for Word2Vec. You will be using Keras to train your model. \n",
    "\n",
    "The sample code for skip-gram model is given. Your job is to incorporate the tokenizer model that you created in HomeWork-1 to tokenize raw text and turn it into word vectors.\n",
    "\n",
    "You must complete the following tasks:\n",
    "1. Read/clean text files\n",
    "2. Indexing (Assign a number to each word)\n",
    "3. Create skip-grams (inputs for your model)\n",
    "4. Create the skip-gram neural network model\n",
    "5. Visualization\n",
    "6. Evaluation (Using pre-trained, not using pre-trained)\n",
    "    (classify topic from 4 categories) \n",
    "    \n",
    "This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n",
    "\n",
    "As a reminder,\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import GRU, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read/clean text files\n",
    "\n",
    "The given code can be used to processed the pre-tokenzied text file from the wikipedia corpus. In your homework, you must replace those text files with raw text files.  You must use your own tokenizer to process your text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: read the wikipedia text file\n",
    "cwd = os.getcwd()\n",
    "with open(cwd+\"/corpora/wiki/thwiki_chk.txt\") as f:\n",
    "    raw_text = [] \n",
    "    #The text file is already tokenized BUT...\n",
    "    #we've replaced all the spaces between words, so you have to use your tokenizer.\n",
    "    raw_text.extend(re.sub(r\"\\s+\",\"\",f.read()))\n",
    "    #since the wiki file is very large, we will only use 1/20 of the whole wiki file in this homework\n",
    "    # if you have enough memeory and want to add more training data, please feel free to edit this code\n",
    "    # to include more data\n",
    "    raw_text = raw_text[:len(raw_text)//20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a character map\n",
    "CHARS = [\n",
    "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
    "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
    "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
    "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
    "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
    "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
    "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
    "]\n",
    "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}\n",
    "char = np.array(CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_gram_df(df, n_pad):\n",
    "  \"\"\"\n",
    "  Given an input dataframe, create a feature dataframe of shifted characters\n",
    "  Input:\n",
    "  df: timeseries of size (N)\n",
    "  n_pad: the number of context. For a given character at position [idx],\n",
    "    character at position [idx-n_pad/2 : idx+n_pad/2] will be used \n",
    "    as features for that character.\n",
    "  \n",
    "  Output:\n",
    "  dataframe of size (N * n_pad) which each row contains the character, \n",
    "    n_pad_2 characters to the left, and n_pad_2 characters to the right\n",
    "    of that character.\n",
    "  \"\"\"\n",
    "  n_pad_2 = int((n_pad - 1)/2)\n",
    "  for i in range(n_pad_2):\n",
    "      df['char-{}'.format(i+1)] = df['char'].shift(i + 1)\n",
    "      df['char{}'.format(i+1)] = df['char'].shift(-i - 1)\n",
    "  return df[n_pad_2: -n_pad_2]\n",
    "\n",
    "\n",
    "def prepare_wiki_feature(raw_text_input):\n",
    "    \"\"\"\n",
    "    Transform the path to a directory containing processed files \n",
    "    into a feature matrix and output array\n",
    "    \"\"\"\n",
    "    # we use padding equals 21 here to consider 10 characters to the left\n",
    "    # and 10 characters to the right as features for the character in the middle\n",
    "    n_pad = 21\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    pad = [{'char': ' ', 'target': True}]\n",
    "    df_pad = pd.DataFrame(pad * n_pad_2)\n",
    "\n",
    "    df = []\n",
    "\n",
    "    df.append(pd.DataFrame(  {'char': raw_text_input}))\n",
    "\n",
    "    df = pd.concat(df)\n",
    "    # pad with empty string feature\n",
    "    df = pd.concat((df_pad, df, df_pad))\n",
    "\n",
    "    # map characters to numbers, use 'other' if not in the predefined character set.\n",
    "    df['char'] = df['char'].map(lambda x: CHARS_MAP.get(x, 80))\n",
    "\n",
    "    # Use nearby characters as features\n",
    "    df_with_context = create_n_gram_df(df, n_pad=n_pad)\n",
    "\n",
    "    char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
    "             ['char-' + str(i + 1) for i in range(n_pad_2)] + ['char']\n",
    "\n",
    "    # convert pandas dataframe to numpy array to feed to the model\n",
    "    x_char = df_with_context[char_row].as_matrix()\n",
    "\n",
    "    return x_char\n",
    "\n",
    "#A function for displaying our features in text\n",
    "def print_features(tfeature,index):\n",
    "    feature = np.array(tfeature[index],dtype=int).reshape(21,1)\n",
    "    #Convert to string\n",
    "    char_list = char[feature]\n",
    "    left = ''.join(reversed(char_list[10:20].reshape(10))).replace(\" \", \"\")\n",
    "    center = ''.join(char_list[20])\n",
    "    right =  ''.join(char_list[0:10].reshape(10)).replace(\" \", \"\")\n",
    "    word = ''.join([left,' ',center,' ',right])\n",
    "    print(center + ': ' + word )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Homework Question1:</font>\n",
    "<font color='blue'>Use your own tokenizer (aka word segmentation model)  to define word boundaries and split the given text file into words.  Capture the screenshot of your code segment that loads the word segmentation model and uses the model to segment the text files. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO#1 \n",
    "#load your word segmentation model here!\n",
    "def get_your_nn():\n",
    "    #replace \"pass\" with code for your neural net\n",
    "    pass\n",
    "\n",
    "model = get_your_nn()\n",
    "\n",
    "#load weights here/ or alternatively you can also load your entire model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_char= prepare_wiki_feature(raw_text)\n",
    "#feel free to edit prepare_wiki_feature if your model has different input format\n",
    "# As a sanity check, we print out the size of the data.\n",
    "print(' data shape: ', x_char.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_word(raw_text, y_pred):\n",
    "    \"\"\" add spaces between words in the raw text based on your prediction\n",
    "    \"\"\"\n",
    "    split_text=\"\"\n",
    "    for char, y in zip(raw_text,y_pred):\n",
    "        if y == 1:\n",
    "            split_text+=\" \"\n",
    "            split_text+=char\n",
    "        else:\n",
    "            split_text+=char\n",
    "    return split_text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####TOKENIZATION\n",
    "###THIS MIGHT TAKE ABOUT 10 MINS on feed forward models\n",
    "y_pred = model.predict(x_char)\n",
    "prob_to_class = lambda p: 1 if p[0]>=0.5 else 0\n",
    "y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "del x_char #clear up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens= char_to_word(raw_text, y_pred)\n",
    "#print out first 100 words for sanity check\n",
    "print(tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total word count:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Indexing (Assign a number to each word)\n",
    "\n",
    "The code below generates an indexed dataset(each word is represented by a number), a dictionary, a reversed dictionary\n",
    "\n",
    "## <font color='blue'>Homework Question 2:</font>\n",
    "<font color='blue'>“UNK” is often used to represent an unknown word (a word which does not exist in your dictionary/training set). You can also represent a rare word with this token as well.  How do you define a rare word in your program? Explain in your own words and capture the screenshot of your code segment that is a part of this process</font>\n",
    "\n",
    " + <font color='blue'>edit or replace create_index with your own code to set a threshold for rare words and replace them with \"UNK\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2:Build dictionary and build a dataset(replace each word with its index)\n",
    "def create_index(input_text):\n",
    "    # TODO#2:edit or replace this function\n",
    "    words = [word for word in input_text ]\n",
    "    word_count = list()\n",
    "    #use set and len to get the number of unique words\n",
    "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "    #include a token for unknown word\n",
    "    word_count.append((\"UNK\",0))\n",
    "    #print out 10 most frequent words\n",
    "    print(word_count[:10])\n",
    "    dictionary = dict()\n",
    "    dictionary[\"for_keras_zero_padding\"] = 0\n",
    "    for word in word_count:\n",
    "        dictionary[word[0]] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    data = list()\n",
    "    for word in input_text:\n",
    "        data.append(dictionary[word])\n",
    "\n",
    "    return data,dictionary, reverse_dictionary\n",
    "\n",
    "dataset,dictionary, reverse_dictionary=create_index(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output sample (dataset):\",dataset[:10])\n",
    "print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:10]})\n",
    "print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Create skip-grams (inputs for your model)\n",
    "Keras has a skipgrams-generator, the cell below shows us how it generates skipgrams \n",
    "\n",
    "## <font color='blue'>Homework Question 3:</font>\n",
    "<font color='blue'>The negative samples are sampled from sampling_table.  Look through Keras source code to find out how they sample negative samples. Discuss the sampling technique taught in class and compare it to the Keras source code.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create data samples\n",
    "vocab_size = len(dictionary)\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "\n",
    "sample_set= dataset[:10]\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "#TO DO#3 check out keras source code and find out how their sampling technique works. Describe it in your own words.\n",
    "couples, labels = skipgrams(sample_set, vocab_size, window_size=skip_window, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples, labels)\n",
    "\n",
    "for i in range(8):\n",
    "    print(reverse_dictionary[couples[i][0]],reverse_dictionary[couples[i][1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: create the skip-gram model\n",
    "## <font color='blue'>Homework Question 4:</font>\n",
    " <font color='blue'>Q4:  In your own words, discuss why Sigmoid is chosen as the activation function in the  skip-gram model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
    "dim_embedddings = 32\n",
    "V= len(dictionary)\n",
    "\n",
    "#step1: select the embedding of the target word from W\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(V, dim_embedddings)(w_inputs)\n",
    "\n",
    "#step2: select the embedding of the context word from C\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c  = Embedding(V, dim_embedddings)(c_inputs)\n",
    "\n",
    "#step3: compute the dot product:c_k*v_j\n",
    "o = Dot(axes=2)([w, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "\n",
    "#step4: normailize dot products into probability\n",
    "o = Activation('sigmoid')(o)\n",
    "#TO DO#4 Question: Why sigmoid?\n",
    "\n",
    "SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
    "SkipGram.summary()\n",
    "opt=Adam(lr=0.01)\n",
    "SkipGram.compile(loss='binary_crossentropy', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    prev_i=0\n",
    "    #it is likely that your GPU won't be able to handle 36 million words\n",
    "    #just do it 100000 words at a time\n",
    "    \n",
    "    for i in range(len(dataset)//100000):\n",
    "        #generate skipgrams\n",
    "        data, labels = skipgrams(sequence=dataset[prev_i*100000:(i*100000)+100000], vocabulary_size=V, window_size=2, negative_samples=4.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss = SkipGram.train_on_batch(x, y)\n",
    "        prev_i = i \n",
    "        print(loss,i*100000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkipGram.save_weights('/data/my_skipgram32_weights-hw.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get weight of the embedding layer\n",
    "final_embeddings=SkipGram.get_weights()[0]\n",
    "print(final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Intrinsic Evaluation: Word Vector Analogies\n",
    "## <font color='blue'>Homework Question 5: </font>\n",
    "<font color='blue'> Read section 2.1 and 2.3 in this [lecture note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf). Come up with 10 semantic analogy examples and report results produced by your word embeddings </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO#5:Come up with 10 semantic analogy examples and report results produced by your word embeddings \n",
    "#and tell us what you observe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Extrinsic Evaluation\n",
    "\n",
    "## <font color='blue'>Homework Question6:</font>\n",
    "<font color='blue'>\n",
    "Use the word embeddings from the skip-gram model as pre-trained weights in a classification model. Compare the result the with the same classification model that does not use the pre-trained weights. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_filepath = glob.glob('/home/burin/corpora/BEST2010_RAW/training_set/news/*.txt')\n",
    "all_novel_filepath = glob.glob('/home/burin/corpora/BEST2010_RAW/training_set/novel/*.txt')\n",
    "all_article_filepath = glob.glob('/home/burin/corpora/BEST2010_RAW/training_set/article/*.txt')\n",
    "all_encyclopedia_filepath = glob.glob('/home/burin/corpora/BEST2010_RAW/training_set/encyclopedia/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data for the classificaiton model\n",
    "#In your homework, we will only use the first 2000 words in each text file\n",
    "#any text file that has less than 2000 words will be padded\n",
    "#reason:just to make this homework feasible under limited time and resource\n",
    "max_length = 2000\n",
    "def word_to_index(word):\n",
    "    if word in dictionary:\n",
    "        return dictionary[word]\n",
    "    else:#if unknown\n",
    "        return dictionary[\"UNK\"]\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    input_text = list()\n",
    "    for textfile_path in [all_news_filepath, all_novel_filepath, all_article_filepath, all_encyclopedia_filepath]:\n",
    "        for input_file in textfile_path:\n",
    "            f = open(input_file,\"r\") #open file with name of \"*.txt\"\n",
    "            text = re.sub(r'\\|', ' ', f.read()) # replace separation symbol with white space           \n",
    "            text = re.sub(r'<\\W?\\w+>', '', text)# remove <NE> </NE> <AB> </AB> tags\n",
    "            text = text.split() #split() method without an argument splits on whitespace \n",
    "            indexed_text = list(map(lambda x:word_to_index(x), text[:max_length])) #map raw word string to its index   \n",
    "            if 'news' in input_file:\n",
    "                input_text.append([indexed_text,0]) \n",
    "            elif 'novel' in input_file:\n",
    "                input_text.append([indexed_text,1]) \n",
    "            elif 'article' in input_file:\n",
    "                input_text.append([indexed_text,2]) \n",
    "            elif 'encyclopedia' in input_file:\n",
    "                input_text.append([indexed_text,3]) \n",
    "            \n",
    "            f.close()\n",
    "    random.shuffle(input_text)\n",
    "    return input_text\n",
    "\n",
    "input_data = prep_data()\n",
    "train_data = input_data[:int(len(input_data)*0.6)]\n",
    "val_data = input_data[int(len(input_data)*0.6):int(len(input_data)*0.8)]\n",
    "test_data = input_data[int(len(input_data)*0.8):]\n",
    "\n",
    "train_input = [data[0] for data in train_data]\n",
    "train_input = sequence.pad_sequences(train_input, maxlen=max_length) #padding\n",
    "train_target = [data[1] for data in train_data]\n",
    "train_target=to_categorical(train_target, num_classes=4)\n",
    "\n",
    "\n",
    "\n",
    "val_input = [data[0] for data in val_data]\n",
    "val_input = sequence.pad_sequences(val_input, maxlen=max_length) #padding\n",
    "val_target = [data[1] for data in val_data]\n",
    "val_target=to_categorical(val_target, num_classes=4)\n",
    "\n",
    "test_input = [data[0] for data in test_data]\n",
    "test_input = sequence.pad_sequences(test_input, maxlen=max_length) #padding\n",
    "test_target = [data[1] for data in test_data]\n",
    "test_target=to_categorical(test_target, num_classes=4)\n",
    "\n",
    "del input_data, val_data,train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the classification model\n",
    "#TO DO#6 find out how to initialize your embedding layer with pre-trained weights, evaluate and observe\n",
    "#don't forget to compare it with the same model that does not use pre-trained weights\n",
    "#you can use your own model too! and feel free to customize this model as you wish\n",
    "cls_model = Sequential()\n",
    "cls_model.add(Embedding(len(dictionary), 32, input_length=max_length,mask_zero=True))\n",
    "cls_model.add(GRU(32))\n",
    "cls_model.add(Dropout(0.5))\n",
    "cls_model.add(Dense(4, activation='softmax'))\n",
    "opt=Adam(lr=0.01)\n",
    "cls_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "cls_model.summary()\n",
    "print('Train...')\n",
    "cls_model.fit(train_input, train_target,\n",
    "          epochs=10,\n",
    "          validation_data=[val_input, val_target])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
