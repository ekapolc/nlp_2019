{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value Attention Mechanism Homework on Keras: Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
    "\n",
    "In this homework, you will create an MT model with key-value attention mechnism that coverts names of Thai 2019 MP candidates from Thai script to Roman(Latin) script. E.g. นิยม-->niyom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "We have generated a toy dataset using names of Thai MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
    "\n",
    "<img src=\"dataset_diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('mp_name_th_en.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    name_th = []\n",
    "    name_en = []\n",
    "    for row in readCSV:\n",
    "        temp_th = row[0]\n",
    "        temp_en = row[1]\n",
    "\n",
    "        name_th.append(temp_th)\n",
    "        name_en.append(temp_en)\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไกรสีห์ kraisi\n",
      "พัชรี phatri\n",
      "ธีระ thira\n",
      "วุฒิกร wutthikon\n",
      "ไสว sawai\n",
      "สัมภาษณ์  samphat\n",
      "วศิน wasin\n",
      "ทินวัฒน์ thinwat\n",
      "ศักดินัย sakdinai\n",
      "สุรศักดิ์ surasak\n"
     ]
    }
   ],
   "source": [
    "for th, en in zip(name_th[:10],name_en[:10]):\n",
    "    print(th,en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Preprocess dataset for Keras (1 point)\n",
    "* 2 dictionaries for indexing (1 for input and another for output)\n",
    "* DON'T FORGET TO INCLUDE special token for padding\n",
    "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)\n",
    "* Be mindful of your pad_sequences \"padding\" hyperparameter. Choose wisely (post-padding vs pre-padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "## Task 2: Code your own (key-value) attention mechnism (1 point)\n",
    "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
    "* Define global variables\n",
    "* fill code for one_step_attention function\n",
    "* Hint: use keras.layers.Lambda \n",
    "* HINT: you probably gonna need more hidden dimmensions than what you've seen in the demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "from keras.layers import Lambda\n",
    "def softMaxAxis1(x):\n",
    "    return softmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are global variables (shared layers)\n",
    "## Fill your code here\n",
    "## you are allowed to use code in the demo as your template.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "\n",
    "    #Fill code here\n",
    "    return None # return whatever you need to complete this homework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Create and train your encoder/decoder model here (1 point)\n",
    "* HINT: you probably gonna need more hidden dimmensions than what you've seen in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIT YOUR MODEL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai-Script to Roman-Script Translation\n",
    "* Task 4: Test your model on 5 examples of your choice including your name! (1 point)\n",
    "* Task 5: Show your visualization of attention scores on one of your example (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 4\n",
    "#fill your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the attention map\n",
    "* If you need to install thai font: sudo apt install xfonts-thai\n",
    "* this is what your visualization might look like:\n",
    "<img src=\"attn_viz_sample.png\"  style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 5\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family']='Loma' #you can change to other font that works for you\n",
    "#fill your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
