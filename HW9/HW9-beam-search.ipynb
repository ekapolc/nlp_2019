{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW9: Beam Search Decoding - News Headline Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to learn and implement decoding techniques for sequence generation. Usually, the sequence is generated word-by-word from a model. In each step, the model predicted the most likely word based on the predicted words in previous steps (this is called auto-regressive decoding).\n",
    "\n",
    "As such, it is very important how you decide on what to predicted at each step, as it will be conditioned on to predicted all of the following steps. There are two main decoding techniques: **Greedy Decoding** and **Beam Search Decoding**. Greedy Decoding immediately chooses the word with best score at each step, while Beam Search Decoding focuses on the sequence that give the best score overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this exercise, you will need to complete the methods for decoding for a text generation model trained on [New York Times Comments and Headlines dataset](https://www.kaggle.com/aashita/nyt-comments). The model is trained to predict a headline for the news given seed text. You do not need to train any model model in this exercise as we provide both the pretrained model and dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework does not require you to use Google Cloud as the model is quite small (but you can still use it if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to shut down your instance on Gcloud when you are not using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing model and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Reshape, Dropout, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dictionary\n",
    "- Index 0 is empty as it is researved for unknown words\n",
    "- Index 1 is \"eos\", end-of-sentence symbol used for indicating the end of generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "word_to_index = {}\n",
    "\n",
    "with open(\"word_list.txt\", \"r\") as word_list_file:\n",
    "  i = 0\n",
    "  for line in word_list_file:\n",
    "    line = line.strip()\n",
    "    index_to_word[i] = line\n",
    "    word_to_index[line] = i\n",
    "    i += 1\n",
    "\n",
    "total_word_count = len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dict size:\", len(index_to_word))\n",
    "# Sample words\n",
    "for i in range(10):\n",
    "  print(index_to_word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained model\n",
    "- The provided model is built with only a layer of feedforward neural networks. \n",
    "- The model takes a sequence of indices of **5 previously generated words to precited the next one**.\n",
    "- The sequence is padded with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_word_count , 50, input_length=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(total_word_count, activation='softmax'))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we write a function for converting a string to a sequence of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(text, word_to_index):\n",
    "  text = text.strip().split(\" \")\n",
    "  token_list = [word_to_index[x] for x in text]\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, in sequence generation task, the model will continue generating tokens until an end-of-sequence symbol appear or the maximum length is reached. For this task:\n",
    "- The end-of-sequence symbol is \"Eos\" and its index is 1\n",
    "- Use the maximum generation length of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = \"Eos\"\n",
    "eos_index = 1\n",
    "max_gen_length = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1:\n",
    "Now, complete the greedy decoding function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(seed_text, max_gen_length, model, word_to_index, index_to_word, input_len):\n",
    "  \"\"\"Greedy decodes with seed text.\n",
    "\n",
    "  Args:\n",
    "    seed_text: The seed string to be used as initial input to the model.\n",
    "    max_gen_length: Maximum length for generation.\n",
    "                    The decoding process must terminate when this length is reached\n",
    "    model: Pretrained keras model for prediction.\n",
    "    word_to_index: The dictionary for converting word to index.\n",
    "    input_len: A number indicating how many previously generated words will be used as \n",
    "               inputs for the model.\n",
    "  \n",
    "  Your code should do the followings:\n",
    "      1. Convert current_text to sequences of indices by calling texts_to_sequences()\n",
    "      2. Pad the sequence with 0s. You might find pad_sequences() function useful\n",
    "      3. Predict the next token using the model (by calling model.predict() or model.predict_classes())\n",
    "         and choose the token with the highest score as output\n",
    "      4. Convert the predicted index to word and concat it to current_text\n",
    "      5. Return text prediction and a list of probabilities of each step\n",
    "      \n",
    "  You do not need to stop early when end-of-sequence token is generated and can continue decoding\n",
    "  until max_gen_length is reached. We can filter the eos token out later.\n",
    "      \n",
    "  The index is converted back to text after every step purely for simplicity. \n",
    "  When working with real problem you should stick with index until the decoding is done.\n",
    "  But you can always call a function provided by the library so there is no need to implement this yourself.\n",
    "  \"\"\"\n",
    "  current_text = seed_text\n",
    "  for _ in range(max_gen_length):\n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    current_text += \" \" + output_word\n",
    "  return current_text.title(), probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test decoding with seed \"the\"\n",
    "\n",
    "You output must be 'The Alienist Season 1 Episode 2 Darkness Descends Eos Eos In Fall Of Spies For Us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decoding with seed \"the\"\n",
    "greedy_decode(\"the\", max_gen_length, model, word_to_index, index_to_word, input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are serveral end-of-sequence token in your output sequence. \n",
    "### TODO 2:\n",
    "Complete the following function to clean your output and decode with provided seed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(text, eos_token):\n",
    "  \"\"\"Drop eos_token and every words that follow\"\"\"\n",
    "  text = \"\"\n",
    "  pass\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_seeds = [\"to\", \"america\", \"people\", \"next\", \"picture\", \"on\", \"usa\"]\n",
    "for seed in sample_seeds:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be\n",
    "- To Hell With 1979 \n",
    "- America Writer And A Laugher \n",
    "- People To Work Make Them Healthier \n",
    "- Next On The Christie Beat In New Jersey \n",
    "- Picture Trump Obstruct Justice \n",
    "- On The Whole30 Diet Vowing To Eat Smarter Carbs For More Than 30 Days \n",
    "- Usa Gymnastics Still Values Medals More Than Girls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another well-known decoding method is beam search decoding that focuses more on the overall sequence score.\n",
    "\n",
    "Instead of greedily choosing the token with the highest score for each step, beam search decoding expands all possible next tokens and keeps the __k__ most likely sequence at each step, where __k__ is a user-specified beam size. A sequence score is also calculated according user-specified cal_score() function.\n",
    "The beam with the highest score after the decoding process is done will be the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a fews things that you need to know before implementing a beam search decoder:\n",
    "- When eos token is produced, you can stop expanding that beam\n",
    "- However, the ended beams must be sorted together with active beams\n",
    "- The decoding end when every kept beams are either ended or reached the maximum length, but for this task, you can continue decoding until the max_gen_len is reached\n",
    "- We usually work with probability in log scale to avoid numerical underflow. You should use np.log(score) before any calculation\n",
    "- **As probabilities for some classes will be very small, you must add a very small value to the score before taking log e.g np.log(prob + 0.00000001)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Score\n",
    "The naive way to calculate the sequence score is to __multipy every token scores__ together. However, doing so will make the decoder prefer shorter sequence as you multiply the sequence score with a value between \\[0,1\\] for every tokens in the sequence. Thus, we usually normalize the sequence score with its length by calculating its __geometric mean__ instead.\n",
    "\n",
    "### TODO 3:\n",
    "Complete cal_score() function.\n",
    "**You should do this in log scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(score_list, length, normalized=False):\n",
    "  if normalized:\n",
    "    pass\n",
    "  else:\n",
    "    pass\n",
    "  return seq_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete beam_search_decode() according to above description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(seed_text, max_gen_len, model, word_to_index, index_to_word, max_sequence_len, beam_size, normalized=False):\n",
    "  \"\"\"We will do beam search decoing using seed text in this function.\n",
    "    \n",
    "  Output:\n",
    "    beams: A list of top k beams after the decoding ended, each beam is a list of \n",
    "      [seed_text, list of scores, length]\n",
    "\n",
    "  Your code should do the followings:\n",
    "    1.Loop until max_gen_len is reached.\n",
    "    2.During each step, loop thorugh each beam and use it to predict the next word.\n",
    "      If a beam is already ended, continues without expanding.\n",
    "    3.Sort all hypotheses according to cal_score().\n",
    "    4.Keep top k hypotheses to be used at the next step.\n",
    "  \"\"\"\n",
    "  # For each beam we will store (generated text, list of scores, and current length)\n",
    "  # Add initial beam\n",
    "  beams = [[seed_text, [], 0]]\n",
    "  \n",
    "  for _ in range(max_gen_len):\n",
    "    pass\n",
    "\n",
    "  return beams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4 (Coding and Written):\n",
    "Decode with the provided seed texts with beam_size 5 and max_gen_len 10.\n",
    "Compare the results between __greedy, normalized, and unnormalized decoding__.\n",
    "\n",
    "Print a result using greedy decoding and top 2 results using unnormalized and normalized decoing for each seed text.\n",
    "\n",
    "Also, print scores of each candidate according to cal_score(). Use normalization for greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_seeds = [\"to\", \"america\", \"people\", \"next\", \"picture\", \"on\", \"usa\"]\n",
    "for seed in sample_seeds:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your outputs should be\n",
    "```\n",
    "-Greedy-\n",
    "To Hell With 1979  0.99\n",
    "-Unnormalized-\n",
    "To Hell With 1979  0.98\n",
    "To Live In A Nation Of Holers  0.00\n",
    "-Normalized-\n",
    "To Hell With 1979  0.99\n",
    "To Live In A Nation Of Holers  0.39\n",
    "\n",
    "-Greedy-\n",
    "America Writer And A Laugher  0.52\n",
    "-Unnormalized-\n",
    "America Process  0.05\n",
    "America Bb And Pellet Gun Injuries Pose Serious Risk To Childrens 0.04\n",
    "-Normalized-\n",
    "America Bb And Pellet Gun Injuries Pose Serious Risk To Childrens 0.73\n",
    "America Jacket On Smoking Should Us Even Tougher  0.65\n",
    "\n",
    "-Greedy-\n",
    "People To Work Make Them Healthier  0.84\n",
    "-Unnormalized-\n",
    "People To Work Make Them Healthier  0.35\n",
    "People Liberal Democracies Perish  0.03\n",
    "-Normalized-\n",
    "People To Work Make Them Healthier  0.84\n",
    "People To Work Make High Healthier  0.52\n",
    "\n",
    "-Greedy-\n",
    "Next On The Christie Beat In New Jersey  0.91\n",
    "-Unnormalized-\n",
    "Next On The Christie Beat In New Jersey  0.47\n",
    "Next On The Whole30 Diet Vowing To Eat Smarter Carbs For 0.33\n",
    "-Normalized-\n",
    "Next On The Christie Beat In New Jersey  0.91\n",
    "Next On The Whole30 Diet Vowing To Eat Smarter Carbs For 0.90\n",
    "\n",
    "-Greedy-\n",
    "Picture Trump Obstruct Justice  0.48\n",
    "-Unnormalized-\n",
    "Picture Trump Obstruct Justice  0.05\n",
    "Picture Trump Save American Steel  0.05\n",
    "-Normalized-\n",
    "Picture Are Conflict Victims But All Is Not Lost  0.67\n",
    "Picture To Live In A Nation Of Holers  0.65\n",
    "\n",
    "-Greedy-\n",
    "On The Whole30 Diet Vowing To Eat Smarter Carbs For More Than 30 Days  0.90\n",
    "-Unnormalized-\n",
    "On Family Farms Little Hands Steer Big Machines  0.46\n",
    "On The Whole30 Diet Vowing To Eat Smarter Carbs For More 0.25\n",
    "-Normalized-\n",
    "On Family Farms Little Hands Steer Big Machines  0.91\n",
    "On The Whole30 Diet Vowing To Eat Smarter Carbs For More 0.87\n",
    "\n",
    "-Greedy-\n",
    "Usa Gymnastics Still Values Medals More Than Girls  1.00\n",
    "-Unnormalized-\n",
    "Usa Gymnastics Still Values Medals More Than Girls  0.99\n",
    "Usa Longterm Cesarean Risks  0.00\n",
    "-Normalized-\n",
    "Usa Gymnastics Still Values Medals More Than Girls  1.00\n",
    "Usa Finds A Match Helping Police Solve An Infamous 1994 Rape 0.49\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q__: From the ouputs, what is the effect of using length normalization?\n",
    "\n",
    "__Ans__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should be able to tell that in greedy decoding, the output will always be the same if you initialize it with the same seed, regardless how many times you try.\n",
    "\n",
    "This behaviour provides consistency to the output of your model but, at the same time, limits the ability to explore the output space. For example, you might not want the same news headline every times you start with the word \"The\".\n",
    "\n",
    "As such, we will introduce randomness to the model when decoding by using weighted sampling instead of argmax. At every step, we will sample the output using softmax outputs as probabilities for each word. One way you can implement this is random a number between 0 and 1 then loop through the probabilities of each word while iteratively adding it together. When the sum is more than the sampled number, you select that word as an output at that step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might notice that even with the sampling method we just introduced, the output is most likely to be the same as greedy decoding because the probabilities of each word are too different (0.99 vs 0.01). \n",
    "Thus, we will use another method called temperature sampling to smoothen the probilities. Before sampling, we will scale each probabilites by powering it with _1/T_ then divide each value with the sum of all values to make its sum equals to 1 again. \n",
    "\n",
    "$$f_T(p_i) = \\frac{p_i^{1/T}}{\\sum p_i^{1/T}} $$\n",
    "\n",
    "Larger T will make the model more likely to choose unlikely words at each step. If T is close to 0, it will be the same as argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5:\n",
    "Implement greedy decoding function with temperature sampling. This function shoud be almost identical to your greedy_decode() except it does not use argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_output(probs, temperature=1.0):\n",
    "  \"\"\"\n",
    "  probs: an array of probabilities\n",
    "  temperature: temperature\n",
    "  \n",
    "  Return: index of the predicted words\n",
    "  \"\"\"\n",
    "  pass\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling_decode(seed_text, max_gen_length, model, word_to_index, index_to_word, input_len, temperature):\n",
    "  \"\"\"Greedy decodes with seed text using temperature sampling.\n",
    "\n",
    "  Args:\n",
    "    seed_text: The seed string to be used as initial input to the model.\n",
    "    max_gen_length: Maximum length for generation.\n",
    "                    The decoding process must terminate when this length is reached\n",
    "    model: Pretrained keras model for prediction.\n",
    "    word_to_index: The dictionary for converting word to index.\n",
    "    input_len: A number indicating how many previously generated words will be used as \n",
    "               inputs for the model.\n",
    "    temperature: temperature.\n",
    "\n",
    "  The retured probs must be before rescaling.\n",
    "  \"\"\"\n",
    "  current_text = seed_text\n",
    "  probs = []\n",
    "  for _ in range(max_gen_length):\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    current_text += \" \" + output_word\n",
    "  return current_text.title(), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_sampling_decode(\"the\", max_gen_length, model, word_to_index, index_to_word, input_len, 0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same seed texts as above, compare the output from normal greedy decoding to Temperature Sampling with T=1.5 and T=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to shut down your instance on Gcloud when you are not using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
